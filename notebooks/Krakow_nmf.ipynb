{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from auroraPSI.itx_adapter import ItxAdapter\n",
    "from auroraPSI.pandas_plotter_adapter import PlotterAdapter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the original Krakow data in format itx, we resample at different resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "itx_adapter = ItxAdapter.read_file(\"../data/observations/KRK_input_10min.itx\")\n",
    "\n",
    "krakow_dict = itx_adapter.to_pandas()\n",
    "krakow_df = pd.concat([krakow_dict['data'], krakow_dict['errors']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "krakow_df.drop(['12.0','37.0','12.0_err','37.0_err'],axis=1,inplace=True)\n",
    "\n",
    "krakow_df_10min = krakow_df.resample(\"10min\").mean()\n",
    "krakow_df_30min = krakow_df.resample(\"30min\").mean()\n",
    "krakow_df_1h = krakow_df.resample(\"1H\").mean()\n",
    "krakow_df_1d = krakow_df.resample(\"1D\").mean()\n",
    "krakow_df_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plotter = PlotterAdapter(krakow_df_10min)\n",
    "# plotter.scatter_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter.date_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter.histogram_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter.all_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter.all_error_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.signal as signal\n",
    "# import numpy as np\n",
    "# times = krakow_df_10min.index.values.astype(float)\n",
    "# values = krakow_df_10min['41.0']\n",
    "# times = times[~np.isnan(values)]\n",
    "# values = values[~np.isnan(values)]\n",
    "# w= np.linspace(0.001, 60, krakow_df_10min.shape[0])\n",
    "# pgram = signal.lombscargle(times,values, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, (ax_t, ax_w) = plt.subplots(2, 1, constrained_layout=True)\n",
    "# ax_t.plot(times, values, 'b+')\n",
    "# ax_t.set_xlabel('Time [s]')\n",
    "# ax_w.plot(w, pgram)\n",
    "# ax_w.set_xlabel('Angular frequency [rad/s]')\n",
    "# ax_w.set_ylabel('Normalized amplitude')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = krakow_df_10min['41.0']\n",
    "X = krakow_df_10min.values[~np.isnan(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt, nx = X.shape # time, compounds\n",
    "k = 10 #sources 10 to 20\n",
    "\n",
    "nt, nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition._nmf import _initialize_nmf as initialize_nmf \n",
    "from scipy.sparse import lil_matrix, block_diag\n",
    "\n",
    "\n",
    "dicotomy_tol = 1e-10\n",
    "shift = 1e-14\n",
    "sigmaL = 2\n",
    "\n",
    "def dichotomy_simplex(num, denum, tol=dicotomy_tol, maxit=40):\n",
    "    \"\"\"\n",
    "    Function to solve the num/(x+denum) -1 = 0 equation. Here, x is the Lagragian multiplier which is used to apply the simplex constraint.\n",
    "    The first part consists in finding a and b such that num/(a+denum) -1 > 0 and num/(b+denum) -1  < 0. \n",
    "    The second part applies the dichotomy algorithm to solve the equation.\n",
    "    \"\"\"\n",
    "    # The function has exactly one root at the right of the first singularity (the singularity at min(denum))\n",
    "    \n",
    "    # num = num.astype(\"float64\")\n",
    "    # denum = denum.astype(\"float64\")\n",
    "\n",
    "    # do some test\n",
    "\n",
    "    assert((num>=0).all())\n",
    "    assert((denum>=0).all())\n",
    "    assert((np.sum(num, axis=0)>0).all())\n",
    "    \n",
    "    # Ideally we want to do this, but we have to exclude the case where num==0.\n",
    "    # a = np.max(num/2 - denum, axis=0)\n",
    "    if denum.shape[1]>1:\n",
    "        a = []\n",
    "        for n,d in zip(num.T, denum.T):\n",
    "            m = n>0\n",
    "            a.append(np.max(n[m]/2 - d[m]))\n",
    "        a = np.array(a)\n",
    "    else:\n",
    "        d = denum[:,0]\n",
    "        def max_masked(n):\n",
    "            m = n>0\n",
    "            return np.max(n[m]/2-d[m])\n",
    "        a = np.apply_along_axis(max_masked, 0, num)\n",
    "        \n",
    "        \n",
    "    # r = np.sum(num/denum, axis=0)\n",
    "    # b = np.zeros(r.shape)\n",
    "    # b[r>=1] = (len(num) * np.max(num, axis=0)/0.5 - np.min(denum, axis=0))[r>=1]\n",
    "    \n",
    "    b = len(num) * np.max(num, axis=0)/0.5 - np.min(denum, axis=0)\n",
    "\n",
    "    def func(x):\n",
    "        return np.sum(num / (x + denum), axis=0) - 1\n",
    "    func_a = func(a)\n",
    "    func_b = func(b)\n",
    "    \n",
    "    assert(np.sum(func_b>=0)==0)\n",
    "    assert(np.sum(func_a<=0)==0)\n",
    "    assert(np.sum(np.isnan(func_a))==0)\n",
    "    assert(np.sum(np.isnan(func_b))==0)\n",
    "\n",
    "\n",
    "    return dicotomy(a, b, func, maxit, tol)\n",
    "\n",
    "def dicotomy(a, b, func, maxit, tol):\n",
    "    \"\"\"\n",
    "    Dicotomy algorithm searching for func(x)=0.\n",
    "\n",
    "    Inputs:\n",
    "    * a: bound such that func(a) > 0\n",
    "    * b: bound such that func(b) < 0\n",
    "    * maxit: maximum number of iteration\n",
    "    * tol: tolerance - the algorithm stops if |func(sol)| < tol\n",
    "    \n",
    "    This algorithm work for number or numpy array of any size.\n",
    "    \"\"\"\n",
    "    # Dichotomy algorithm to solve the equation\n",
    "    it = 0\n",
    "    new = (a + b)/2\n",
    "    func_new = func(new)\n",
    "    while np.max(np.abs(func_new)) > tol:\n",
    "        \n",
    "        it=it+1\n",
    "        func_a = func(a)\n",
    "        # func_b = func(b)\n",
    "\n",
    "        # if f(a)*f(new) <0 then f(new) < 0 --> store in b\n",
    "        minus_bool = func_a * func_new <= 0\n",
    "        \n",
    "        # if f(a)*f(new) > 0 then f(new) > 0 --> store in a\n",
    "        # plus_bool = func_a * func_new > 0\n",
    "        plus_bool = np.logical_not(minus_bool)\n",
    "\n",
    "        b[minus_bool] = new[minus_bool]\n",
    "        a[plus_bool] = new[plus_bool]\n",
    "        new = (a + b) / 2\n",
    "        func_new = func(new)\n",
    "        if it>=maxit:\n",
    "            print(\"Dicotomy stopped for maximum number of iterations\")\n",
    "            break\n",
    "    return new\n",
    "\n",
    "def Frobenius_loss(X, W, H, average=False):\n",
    "    \"\"\"\n",
    "    Compute the generalized KL divergence.\n",
    "\n",
    "    \\sum_{ji} | X_{ij} - (D H)_{ij} |^2\n",
    "    \"\"\"\n",
    "    \n",
    "    DH = W @ H\n",
    "\n",
    "    if average:\n",
    "        return np.mean((DH - X)**2)\n",
    "    else:\n",
    "        return np.sum((DH - X)**2)\n",
    "\n",
    "def trace_xtLx(L, x, average=False):\n",
    "    if average:\n",
    "        return np.mean(x * (L @ x))\n",
    "    else:\n",
    "        return np.sum(x * (L @ x))\n",
    "    \n",
    "    \n",
    "\n",
    "def create_laplacian_matrix(nx):\n",
    "    \"\"\"\n",
    "    Helper method to create the laplacian matrix for the laplacian regularization\n",
    "    :param n: width of the original image\n",
    "    :return:the n x n laplacian matrix\n",
    "    \"\"\"\n",
    "    assert(nx>1)\n",
    "    #Blocks corresponding to the corner of the image (linking row elements)\n",
    "    mat=lil_matrix((nx,nx),dtype=np.float32)\n",
    "    mat.setdiag(2)\n",
    "    mat.setdiag(-1,k=1)\n",
    "    mat.setdiag(-1,k=-1)\n",
    "    mat[0,0] = 1\n",
    "    mat[nx-1, nx-1] = 1\n",
    "    return mat\n",
    "\n",
    "def multiplicative_step_G(X, F, G, force_simplex=True,  safe=True, dicotomy_tol=dicotomy_tol, shift=shift):\n",
    "    \"\"\"\n",
    "    Multiplicative step in G.\n",
    "    \"\"\"\n",
    "\n",
    "    if safe:\n",
    "        # Allow for very small negative values!\n",
    "        assert(np.sum(G<-shift/2)==0)\n",
    "        assert(np.sum(F<-shift/2)==0)\n",
    "        G = np.maximum(G, shift)\n",
    "        F = np.maximum(F, shift)\n",
    "    \n",
    "    FF = F.T @ F\n",
    "    FX = F.T @ X\n",
    "    num = G * FX\n",
    "    denum = FF @ G\n",
    "\n",
    "\n",
    "    if force_simplex:\n",
    "        nu = dichotomy_simplex(num, denum,dicotomy_tol)\n",
    "    else:\n",
    "        nu = 0\n",
    "        \n",
    "    if safe:\n",
    "        assert np.sum(denum<0)==0\n",
    "        assert np.sum(num<0)==0\n",
    "\n",
    "    new_G = num/(denum+nu)\n",
    "    return new_G\n",
    "\n",
    "\n",
    "\n",
    "def multiplicative_step_F(X, F, G, lambda_L=0, L=None, sigmaL=sigmaL, safe=True, shift=shift):\n",
    "    \"\"\"\n",
    "    Multiplicative step in F.\n",
    "    \"\"\"\n",
    "    if not(lambda_L==0):\n",
    "        if L is None:\n",
    "            raise ValueError(\"Please provide the laplacian\")\n",
    "    if safe:\n",
    "        # Allow for very small negative values!\n",
    "        assert(np.sum(G<-shift/2)==0)\n",
    "        assert(np.sum(F<-shift/2)==0)\n",
    "        G = np.maximum(G, shift)\n",
    "        F = np.maximum(F, shift)\n",
    "\n",
    "    GG = G @ G.T\n",
    "    GX = X @ G.T\n",
    "    num = GX\n",
    "    denum = F @ GG  \n",
    "\n",
    "\n",
    "    if not(lambda_L==0):\n",
    "        num = num + lambda_L * sigmaL\n",
    "        denum = denum + lambda_L * sigmaL + lambda_L * L @ F \n",
    "        \n",
    "    num = F * num\n",
    "    \n",
    "    if safe:\n",
    "        assert np.sum(denum<0)==0\n",
    "        assert np.sum(num<0)==0\n",
    "\n",
    "    new_F = num/denum\n",
    "\n",
    "    return new_F\n",
    "\n",
    "def initialize_algorithms(X, F, G, n_components, init, random_state, force_simplex):\n",
    "    # Handle initialization\n",
    "    if F is None:\n",
    "        if G is None:\n",
    "            F, G = initialize_nmf(X, n_components=n_components, init=init, random_state=random_state)\n",
    "            if force_simplex:\n",
    "                scale = np.sum(G, axis=0, keepdims=True)\n",
    "                G = np.nan_to_num(G/scale, nan = 1.0/G.shape[0] )\n",
    "        else:\n",
    "            F = np.abs(np.linalg.lstsq(G, X.T, rcond=None)[0]).T\n",
    "\n",
    "    elif G is None:\n",
    "        G = np.abs(np.linalg.lstsq(F, X, rcond=None)[0])\n",
    "        if force_simplex:\n",
    "            scale = np.sum(G, axis=0, keepdims=True)\n",
    "            G = G/scale\n",
    "\n",
    "    return F, G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "import time\n",
    "\n",
    "class NMFEstimator(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    loss_names_ = [\"Frobenius loss\", \"Laplacian loss\"]\n",
    "\n",
    "    \n",
    "    def __init__(self, n_components=2, init='warn', tol=1e-4, max_iter=200,\n",
    "                 random_state=None, verbose=1, debug=False, lambda_L=0,\n",
    "                shift=shift, eval_print=10, force_simplex=False, dicotomy_tol=dicotomy_tol):\n",
    "        self.n_components = n_components\n",
    "        self.init = init\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.shift = shift\n",
    "        self.debug = debug\n",
    "        self.eval_print = eval_print\n",
    "        self.lambda_L = lambda_L\n",
    "        self.force_simplex = force_simplex\n",
    "        self.dicotomy_tol = dicotomy_tol\n",
    "\n",
    "\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'requires_positive_X': True}\n",
    "\n",
    "    def _iteration(self,  F, G):\n",
    "        F = multiplicative_step_F(self.X_,  F, G, safe=self.debug, lambda_L=self.lambda_L, L=self.L_, shift=self.shift)\n",
    "        G = multiplicative_step_G(self.X_, F, G, force_simplex=self.force_simplex, safe=self.debug, dicotomy_tol=self.dicotomy_tol, shift=self.shift)\n",
    "        return F, G\n",
    "    \n",
    "\n",
    "    def loss(self, F, G, average=True, X = None):\n",
    "        if X is None : \n",
    "            X = self.X_\n",
    "\n",
    "        assert(X.shape == (F.shape[0],G.shape[1]))\n",
    "\n",
    "        self.FG_numel_ = F.shape[0] * G.shape[1]\n",
    "        \n",
    "        loss_fro = Frobenius_loss(X, F, G, average=average)\n",
    "        loss = loss_fro\n",
    "        self.detailed_loss_ = [loss_fro, 0]\n",
    "\n",
    "        if self.lambda_L>0:\n",
    "            loss_tr =  self.lambda_L * trace_xtLx(self.L_, F, average=average)\n",
    "            loss += loss_tr\n",
    "            self.detailed_loss_[1] = loss_tr\n",
    "        \n",
    "        # if average:\n",
    "        #     loss = loss / self.FG_numel_\n",
    "        return loss\n",
    "\n",
    "    def fit_transform(self, X, y=None, F=None, G=None):\n",
    "        \"\"\"Learn a NMF model for the data X and returns the transformed data.\n",
    "        This is more efficient than calling fit followed by transform.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Data matrix to be decomposed\n",
    "        F : array-like of shape (n_samples, n_components)\n",
    "            If specified, it is used as initial guess for the solution.\n",
    "        G : array-like of shape (n_components, n_features)\n",
    "            If specified, it is used as initial guess for the solution.\n",
    "        Returns\n",
    "        -------\n",
    "        F,G : ndarrays\n",
    "        \"\"\"\n",
    "\n",
    "        self.X_ = self._validate_data(X, dtype=[np.float64, np.float32])\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "        self.X_ = self.remove_zeros_lines(self.X_, self.shift)\n",
    "\n",
    "        \n",
    "        self.F_, self.G_ = initialize_algorithms(X = self.X_, F= F, G = G, n_components = self.n_components, init = self.init, random_state = self.random_state, force_simplex = self.force_simplex)\n",
    "\n",
    "        if self.lambda_L > 0:\n",
    "            self.L_ = create_laplacian_matrix(self.F_.shape[0])\n",
    "        else:\n",
    "            self.L_ = None\n",
    "\n",
    "\n",
    "        algo_start = time.time()\n",
    "        # If mu_sparse != 0, this is the regularized step of the algorithm\n",
    "        # Otherwise this is directly the data fitting step\n",
    "        eval_before = np.inf\n",
    "        eval_init = self.loss(self.F_, self.G_)\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "        # if self.debug:\n",
    "        self.losses_ = []\n",
    "        self.rel_ = []\n",
    "        self.detailed_losses_ = []\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Take one step in A, P\n",
    "                old_F, old_G = self.F_.copy(), self.G_.copy()\n",
    "                \n",
    "                self.F_, self.G_ = self._iteration(self.F_, self.G_ )\n",
    "                eval_after = self.loss(self.F_, self.G_)\n",
    "                self.n_iter_ +=1\n",
    "                \n",
    "                rel_F = np.max((self.F_ - old_F)/(self.F_ + self.tol*np.mean(self.F_) ))\n",
    "                rel_G = np.max((self.G_ - old_G)/(self.G_ + self.tol*np.mean(self.G_) ))\n",
    "\n",
    "                # store some information for assessing the convergence\n",
    "                # for debugging purposes\n",
    "                detailed_loss_ = self.detailed_loss_\n",
    "\n",
    "                self.losses_.append(eval_after)\n",
    "                self.detailed_losses_.append(detailed_loss_)\n",
    "                self.rel_.append([rel_F,rel_G])\n",
    "                              \n",
    "                # check convergence criterions\n",
    "                if self.n_iter_ >= self.max_iter:\n",
    "                    print(\"exits because max_iteration was reached\")\n",
    "                    break\n",
    "\n",
    "                # If there is no regularization the algorithm stops with this criterion\n",
    "                # Otherwise it goes to the data fitting step\n",
    "                elif max(rel_F,rel_G) < self.tol:\n",
    "                    print(\n",
    "                        \"exits because of relative change rel_F {} or rel_G {} < tol \".format(\n",
    "                            rel_F,rel_G\n",
    "                        )\n",
    "                    )\n",
    "                    break\n",
    "                elif abs((eval_before - eval_after)/eval_init) < self.tol:\n",
    "                    print(\n",
    "                        \"exits because of relative change < tol: {}\".format(\n",
    "                            (eval_before - eval_after)/min(eval_before, eval_after)\n",
    "                        )\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                elif np.isnan(eval_after):\n",
    "                    print(\"exit because of the presence of NaN\")\n",
    "                    break\n",
    "\n",
    "                elif (eval_before - eval_after) < 0:\n",
    "                    if hasattr(self, \"accelerate\"):\n",
    "                        if not self.accelerate:\n",
    "                            print(\"exit because of negative decrease {}: {}, {}\".format((eval_before - eval_after), eval_before, eval_after))\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"exit because of negative decrease {}: {}, {}\".format((eval_before - eval_after), eval_before, eval_after))\n",
    "                        break\n",
    "                \n",
    "                if self.verbose > 0 and np.mod(self.n_iter_, self.eval_print) == 0:\n",
    "                    print(\n",
    "                        f\"It {self.n_iter_} / {self.max_iter}: loss {eval_after:0.3f},  {self.n_iter_/(time.time()-algo_start):0.3f} it/s\",\n",
    "                    )\n",
    "                    pass\n",
    "                eval_before = eval_after\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "        # if not(self.force_simplex):\n",
    "        #     self.F_, self.G_ = rescaled_DH(self.F_, self.G_ )\n",
    "        \n",
    "        algo_time = time.time() - algo_start\n",
    "        print(\n",
    "            f\"Stopped after {self.n_iter_} iterations in {algo_time//60} minutes \"\n",
    "            f\"and {np.round(algo_time) % 60} seconds.\"\n",
    "        )\n",
    "        self.reconstruction_err_ = self.loss(self.F_, self.G_)\n",
    "        \n",
    "        self.n_components_ = self.G_.shape[0]\n",
    "        self.components_ = self.G_\n",
    "        return self.F_\n",
    "\n",
    "    def fit(self, X, y=None, **params):\n",
    "        \"\"\"Learn a NMF model for the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Data matrix to be decomposed\n",
    "        y : Ignored\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, **params)\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, F):\n",
    "        \"\"\"Transform data back to its original space.\n",
    "        Parameters\n",
    "        ----------\n",
    "        W : {ndarray, sparse matrix} of shape (n_samples, n_components)\n",
    "            Transformed data matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
    "            Data matrix of original shape.\n",
    "        .. versionadded:: 0.18\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        return  F @ self.G_\n",
    "    \n",
    "    def get_losses(self):\n",
    "        \n",
    "        names = [\"full_loss\"] + self.loss_names_ + [\"rel_W\",\"rel_H\"]\n",
    "        dt_list = []\n",
    "        for elt in names : \n",
    "            dt_list.append((elt,\"float64\"))\n",
    "        dt = np.dtype(dt_list)\n",
    "\n",
    "        tup_list = []\n",
    "        for i in range(len(self.losses_)) : \n",
    "            tup_list.append((self.losses_[i],) + tuple(self.detailed_losses_[i]) + tuple(self.rel_[i]))\n",
    "        \n",
    "        array = np.array(tup_list,dtype=dt)\n",
    "\n",
    "        return array\n",
    "\n",
    "    def remove_zeros_lines (self, X, epsilon) : \n",
    "        if np.all(X >= 0) : \n",
    "            new_X = X.copy()\n",
    "            sum_cols = X.sum(axis = 0)\n",
    "            sum_rows = X.sum(axis = 1)\n",
    "            new_X[:,np.where(sum_cols == 0)] = epsilon\n",
    "            new_X[np.where(sum_rows == 0),:] = epsilon\n",
    "            return new_X\n",
    "        else : \n",
    "            raise ValueError(\"There are negative values in X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(X[:200])\n",
    "plt.colorbar()\n",
    "np.min(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[:200]<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "est = NMFEstimator(n_components=k\n",
    "             , init='random', tol=1e-4, max_iter=200,\n",
    "                 random_state=None, verbose=1, debug=False, lambda_L=3,\n",
    "                shift=shift, eval_print=10, force_simplex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = X.copy()\n",
    "Xnew[X<0] = 0\n",
    "est.fit(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = est.F_\n",
    "G = est.G_\n",
    "loss = est.get_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(G, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mark_space = 20\n",
    "fontsize = 15\n",
    "marker_list = [\"-o\",\"-s\",\"->\",\"-<\",\"-^\",\"-v\",\"-d\"]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "names = list(loss.dtype.names)\n",
    "for j,name in enumerate(names) :\n",
    "    if re.match(r\".*(loss)\",name) : \n",
    "        axes[0].plot(loss[name],marker_list[j%len(marker_list)],markersize=3.5,label = name,markevery = mark_space,linewidth = 2)\n",
    "        axes[0].set_yscale(\"log\")\n",
    "        axes[0].legend()\n",
    "        axes[0].set_xlabel(\"number of iterations\")\n",
    "    elif re.match(r\"^(rel)\",name) : \n",
    "        axes[1].plot(loss[name],marker_list[j%len(marker_list)],markersize=3.5,label = name,markevery = mark_space,linewidth = 2)\n",
    "        axes[1].legend()\n",
    "        axes[1].set_xlabel(\"number of iterations\")\n",
    "    # elif re.match(r\"^(ang)\",name) :\n",
    "    #     axes[2].plot(loss[name],marker_list[j%len(marker_list)],markersize=3.5,label = name,markevery = mark_space,linewidth = 2)\n",
    "    #     axes[2].legend()\n",
    "    #     axes[2].set_xlabel(\"number of iterations\")\n",
    "    # elif re.match(r\"^(mse)\",name) :\n",
    "    #     axes[3].plot(loss[name],marker_list[j%len(marker_list)],markersize=3.5,label = name,markevery = mark_space,linewidth = 2)\n",
    "    #     axes[3].legend()\n",
    "    #     axes[3].set_xlabel(\"number of iterations\")\n",
    "\n",
    "cols = [\"Losses\", \"Evolution of A and P\",\"Angles\",\"MSE\"]\n",
    "\n",
    "for ax, col in zip(axes, cols):\n",
    "    ax.set_title(col, fontsize=fontsize)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(G)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3595a144e2725957a34b98bbe5f9ab9d89d56d08f171010103952858f181e49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('aurora-qeRysJBD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
